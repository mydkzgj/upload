# UTR Function Prediction Tutorial

```python
import sys
sys.path.append('./')
sys.path.append('../')
sys.path.append('../..')

import os
import pandas as pd
from sklearn import preprocessing
import string
from typing import Sequence, Tuple, List, Union
import fm
import torch
from torch import nn
from torch import optim
from torch.utils.data import DataLoader
from torch.utils.data.sampler import RandomSampler
import numpy as np
```

# 1. Load Model


```python
class DownStreamModule(nn.Module):
    """
    base contact predictor for msa
    """
    def __init__(self, backbone_args, backbone_alphabet, depth_reduction="none",
                 need_token=False, need_attention=[], need_embedding=[12], need_extrafeat=[]):
        super().__init__()
        self.backbone_args = backbone_args
        self.backbone_alphabet = backbone_alphabet

        self.prepend_bos = self.backbone_alphabet.prepend_bos
        self.append_eos = self.backbone_alphabet.append_eos
        self.bos_idx = self.backbone_alphabet.cls_idx
        self.eos_idx = self.backbone_alphabet.eos_idx
        if self.append_eos and self.eos_idx is None:
            raise ValueError("Using an alphabet with eos token, but no eos token was passed in.")
        self.pad_idx = self.backbone_alphabet.padding_idx

        self.embed_dim = self.backbone_args.embed_dim
        self.attention_heads = self.backbone_args.attention_heads

        self.depth_reduction = depth_reduction
        if self.depth_reduction == "attention":
            self.msa_embed_dim_in = self.embed_dim
            self.msa_embed_dim_out = self.embed_dim // self.attention_heads
            self.msa_q_proj = nn.Linear(self.msa_embed_dim_in, self.msa_embed_dim_out)
            self.msa_k_proj = nn.Linear(self.msa_embed_dim_in, self.msa_embed_dim_out)

        self.input_type = {
            "token": need_token,
            "attention": need_attention,
            "embedding": need_embedding,
            "extra-feat": need_extrafeat,
        }


    def remove_pend_tokens_1d(self, tokens, seqs):
        """
        :param tokens:
        :param seqs: must be shape of [B, ..., L, E]    # seq: [B, L, E]; msa: [B, D, L, E]
        :return:
        """
        padding_masks = tokens.ne(self.pad_idx)

        # remove eos token  （suffix first）
        if self.append_eos:     # default is right
            eos_masks = tokens.ne(self.eos_idx)
            eos_pad_masks = (eos_masks & padding_masks).to(seqs)
            seqs = seqs * eos_pad_masks.unsqueeze(-1)
            seqs = seqs[:, ..., :-1, :]
            padding_masks = padding_masks[:, ..., :-1]

        # remove bos token
        if self.prepend_bos:    # default is left
            seqs = seqs[:, ..., 1:, :]
            padding_masks = padding_masks[:, ..., 1:]

        if not padding_masks.any():
            padding_masks = None

        return seqs, padding_masks

    

class Human5PrimeUTRPredictor(DownStreamModule): # (torch.nn.Module):
    """
    contact predictor with inner product
    """
    def __init__(self, backbone_args, backbone_alphabet, task="cls", arch="mlp",
                 depth_reduction="mean", need_token=False, need_attention=[], need_embedding=[12],
                 need_extrafeat=["ss-rnafm"]):
        """
        :param depth_reduction: mean, first
        """
        super().__init__(backbone_args, backbone_alphabet, depth_reduction, need_token, need_attention, need_embedding, need_extrafeat)
        self.embed_dim_in = self.backbone_args.embed_dim
        self.attention_heads = self.backbone_args.attention_heads

        self.task = task
        self.arch = arch

        self.padding_mode = "right" #"left"

        self.token_len = 100

        self.out_plane = 1

        self.in_channels = 0
        if self.input_type["token"] == True:
            self.in_channels = self.in_channels + 4

        if self.input_type["embedding"] != []:
            self.reductio_module = nn.Linear(640, 32)   #nn.Conv1d(640, 16, kernel_size=1)
            self.in_channels = self.in_channels + 32  #640

        if self.arch == "mlp":
            self.predictor = self.create_mlp(self.in_channels, out_labels=1)
        elif self.arch == "cnn":
            #self.predictor = self.create_cnn(in_channels=self.in_channels, in_token_len=self.token_len)
            if self.in_channels > 0:
                self.predictor = self.create_1dcnn_for_emd(in_planes=self.in_channels, out_planes=1)

    def forward(self, tokens, inputs):
        ensemble_inputs = []
        if "token" in inputs:
            nest_token = (tokens[:, 1:-1] - 4)

            # 100 right padding
            nest_token = torch.nn.functional.pad(nest_token, (0, self.token_len - nest_token.shape[1]), value=-1)
            token_padding_mask = nest_token.ge(0).long()
            one_hot_tokens = torch.nn.functional.one_hot((nest_token * token_padding_mask),
                                                         num_classes=4).float() * token_padding_mask.unsqueeze(-1)
            # for 50 fixed length
            # one_hot_tokens = torch.nn.functional.one_hot(nest_token, num_classes=4).float()

            if self.arch == "cnn":     # B, L, 4
                one_hot_tokens = one_hot_tokens.permute(0, 2, 1)
            else:
                one_hot_tokens = one_hot_tokens.view(one_hot_tokens.shape[0], -1)
            ensemble_inputs.append(one_hot_tokens)

        if "embedding" in inputs:
            embeddings = inputs["embedding"]
            # remove auxiliary tokens
            embeddings, padding_masks = self.remove_pend_tokens_1d(tokens, embeddings)
            if len(embeddings.size()) == 3:  # for pure seq
                batch_size, seqlen, hiddendim = embeddings.size()
            elif len(embeddings.size()) == 4:  # for msa
                batch_size, depth, seqlen, hiddendim = embeddings.size()
                # reduction
                embeddings = self.msa_depth_reduction(embeddings, padding_masks)
            else:
                raise Exception("Unknown Embedding Type!")

            # 100 right padding
            embeddings = torch.nn.functional.pad(embeddings, (0, 0, 0, self.token_len - embeddings.shape[1]))

            #embeddings = embeddings.permute(0, 2, 1)
            embeddings = self.reductio_module(embeddings)
            #embeddings = embeddings.permute(0, 2, 1)

            #embeddings = torch.mean(embeddings, dim=-1, keepdim=True)

            if self.arch == "cnn":
                embeddings = embeddings.permute(0, 2, 1)
            else:
                embeddings = embeddings.reshape(embeddings.shape[0], -1)

            ensemble_inputs.append(embeddings)
        

        if len(ensemble_inputs) > 0:
            ensemble_inputs = torch.cat(ensemble_inputs, dim=1)

        if self.padding_mode == "left":
            # 100 left padding
            nest_token = (tokens[:, 1:-1] - 4)
            nest_token = torch.nn.functional.pad(nest_token, (0, self.token_len - nest_token.shape[1]), value=-1)
            token_padding_mask = nest_token.ge(0).long()

            left_ensembles = []
            for i in range(nest_token.shape[0]):
                length = token_padding_mask[i].sum().item()
                left_ensemble = torch.cat([ensemble_inputs[i, :, length:], ensemble_inputs[i, :, 0: length]], dim=-1)
                left_ensembles.append(left_ensemble)
            ensemble_inputs = torch.stack(left_ensembles, dim=0)

        if isinstance(ensemble_inputs, list) != True:
            output = self.predictor(ensemble_inputs)
            output = output.squeeze(-1)
        else:
            output = 0


        return output


    def create_mlp(self, in_planes, out_labels=3, dropout=0.2,):
        mlp = nn.Sequential(
            nn.Flatten(),
            nn.Linear(in_planes, 128),
            nn.ReLU(inplace=True),
            nn.BatchNorm1d(128),
            nn.Dropout(dropout),
            nn.Linear(128, 64),
            nn.ReLU(inplace=True),
            nn.BatchNorm1d(64),
            nn.Dropout(dropout),
            nn.Linear(64, 32),
            nn.ReLU(inplace=True),
            nn.BatchNorm1d(32),
            nn.Dropout(dropout),
            nn.Linear(32, out_labels),
        )
        return mlp

 
    def create_1dcnn_for_emd(self, in_planes, out_planes):
        main_planes = 64
        dropout = 0.2 #0.2  # 0.2
        emb_cnn = nn.Sequential(
            nn.Conv1d(in_planes, main_planes, kernel_size=3, padding=1),  ## 3
            # 1 * 148 * 148 --> mp * 144 * 144   Receptive Field: 5, j=1
            # nn.BatchNorm1d(main_planes),
            # nn.ReLU(inplace=True),         # pre-activation mode in resblock
            # nn.AvgPool1d(kernel_size=2, stride=2, padding=0),                          # mp * 144 * 144 --> mp * 72 * 72    Receptive Field: 6, j=2
            # nn.MaxPool1d(kernel_size=2, stride=2),
            # nn.Dropout(dropout),

            ResBlock(main_planes * 1, main_planes * 1, stride=2, dilation=1, conv_layer=nn.Conv1d,
                     norm_layer=nn.BatchNorm1d),  # mp * 72 * 72  Receptive Field: +2*(3-1)*j= +8=14, j=2    ## 9  j=2
            ResBlock(main_planes * 1, main_planes * 1, stride=1, dilation=1, conv_layer=nn.Conv1d,
                     norm_layer=nn.BatchNorm1d),  # mp * 72 * 72  Receptive Field: +2*(3-1)*j= +8=22, j=2    ## 17  j=2
            # nn.AvgPool1d(kernel_size=2, stride=2),                                                      # mp * 72 * 72 --> mp * 36 * 36  Receptive Field:           +1*2=24, j=4
            # ResBlock(main_planes, main_planes*2, stride=2, dilation=1, conv_layer=nn.Conv1d, norm_layer=nn.BatchNorm1d),
            # nn.MaxPool1d(kernel_size=2, stride=2),
            # nn.Dropout(dropout),

            ResBlock(main_planes * 1, main_planes * 1, stride=2, dilation=1, conv_layer=nn.Conv1d,
                     norm_layer=nn.BatchNorm1d),  # mp * 36 * 36  Receptive Field: +2*(3-1)*j=+16=40, j=4    ## 29  j=4
            ResBlock(main_planes * 1, main_planes * 1, stride=1, dilation=1, conv_layer=nn.Conv1d,
                     norm_layer=nn.BatchNorm1d),  # mp * 36 * 36  Receptive Field: +2*(3-1)*j=+16=56, j=4    ## 45 j=4
            # ResBlock(main_planes*4, main_planes*4, stride=1, dilation=1, conv_layer=nn.Conv1d, norm_layer=nn.BatchNorm1d),
            # nn.AvgPool1d(kernel_size=2, stride=2),                                                      # mp * 36 * 36 --> mp * 18 * 18  Receptive Field:           +1*4=60, j=8
            # ResBlock(main_planes*2, main_planes*4, stride=2, dilation=1, conv_layer=nn.Conv1d, norm_layer=nn.BatchNorm1d),
            # nn.MaxPool1d(kernel_size=2, stride=2),
            # nn.Dropout(dropout),

            ResBlock(main_planes * 1, main_planes * 1, stride=2, dilation=1, conv_layer=nn.Conv1d,
                     norm_layer=nn.BatchNorm1d),  # mp * 18 * 18  Receptive Field: +2*(3-1)*j=+32=92, j=8    ## 69 j=8
            ResBlock(main_planes * 1, main_planes * 1, stride=1, dilation=1, conv_layer=nn.Conv1d,
                     norm_layer=nn.BatchNorm1d),  # mp * 18 * 18  Receptive Field: +2*(3-1)*j=+32=124, j=8   ## 101 j=8
            # ResBlock(main_planes*8, main_planes*8, stride=1, dilation=1, conv_layer=nn.Conv1d, norm_layer=nn.BatchNorm1d),  # mp * 18 * 18  Receptive Field: +2*(3-1)*j=+32=124, j=8
            nn.AdaptiveAvgPool1d(1),
            # AvgPool1d(kernel_size=2, stride=2),                                # mp * 18 * 18 --> mp * 9 * 9    Receptive Field:           +1*8=130, j=16

            # nn.Conv1d(main_planes, 1, kernel_size=3, padding=0),                                        # mp * 9 * 9 --> 1 * 7 * 7       Receptive Field:   +(3-1)*j=+32=162, j=16
            nn.Flatten(),
            nn.Dropout(dropout),
            # nn.Linear(main_planes, main_planes//2),
            # nn.ReLU(inplace=True),
            # nn.BatchNorm1d(main_planes//2),
            nn.Linear(main_planes * 1, out_planes),
        )
        return emb_cnn


class ResBlock(nn.Module):
    def __init__(
        self,
        in_planes,
        out_planes,
        stride=1,
        dilation=1,
        conv_layer=nn.Conv2d,
        norm_layer=nn.BatchNorm2d,
    ):
        super(ResBlock, self).__init__()
        # Both self.conv1 and self.downsample layers downsample the input when stride != 1
        self.bn1 = norm_layer(in_planes)
        self.relu1 = nn.ReLU(inplace=True)
        self.conv1 = conv_layer(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation, bias=False)
        #self.dropout = nn.Dropout(p=0.3)    # without ok?
        self.bn2 = norm_layer(out_planes)
        self.relu2 = nn.ReLU(inplace=True)
        self.conv2 = conv_layer(out_planes, out_planes, kernel_size=3, padding=dilation, bias=False)

        if stride > 1 or out_planes != in_planes:   # main path should be have the same change with branch path for the final addition
            self.downsample = nn.Sequential(
                conv_layer(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),
                norm_layer(out_planes),
            )
        else:
            self.downsample = None

        self.stride = stride

    def forward(self, x):
        identity = x

        out = self.bn1(x)
        out = self.relu1(out)
        out = self.conv1(out)
        #out = self.dropout(out)
        out = self.bn2(out)
        out = self.relu2(out)
        out = self.conv2(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity

        return out
```


```python
backbone, alphabet = fm.pretrained.esm1b_rna_t12("../../redevelop/pretrained/RNA-FM_pretrained.pth")
```


```python
task="rgs"
arch="cnn"
input_items = ["seq", "emb-rnafm"]
need_token, need_atten, need_embed, need_extra = False, [], [], []
if "seq" in input_items:
    need_token = True
if "emb-rnafm" in input_items:
    need_embed = [12]

utr_func_predictor = Human5PrimeUTRPredictor(
    backbone.args, backbone_alphabet, task=task, arch=arch,
    need_token=need_token, need_attention=need_atten, need_embedding=need_embed, need_extrafeat=need_extra,
)

criterion = nn.MSELoss()

optimizer = optim.Adam(utr_func_predictor.parameters(), lr=0.001)
```

# 2. Load Data
You should download data from gdrive link: https://drive.google.com/file/d/10zCfOHOaOa__J2AIuZyidZ9sVJ9L11rI/view?usp=sharing and place it in the tutorials/utr-function-prediction

## Pre-define data processing classes


```python
class Human_5Prime_UTR_VarLength(object):
    def __init__(self, root, set_name="train", input_items=["seq", "emb-rnafm"], label_items=["rl"], use_cache=0):
        """
        :param root: root path of dataset - CATH. however not all of stuffs under this root path
        :param data_type: seq, msa
        :param label_type: 1d, 2d
        :param set_name: "train", "valid", "test"
        """
        self.root = root
        self.theme = "rna"
        self.data_type = "seq"
        self.set_name = set_name          
        self.use_cache = 1
        self.cache = {}

        self.input_items = input_items
        if "seq" not in self.input_items:
            self.input_items.append("seq")  # for generate RNA-FM embedding
        self.label_items = label_items
        self.data_items = input_items + label_items  # combine two items

    
        # 1. Create Paths
        self.src_scv_path = os.path.join(self.root, "data", "GSM4084997_varying_length_25to100.csv")        


        # 2. Create Data INFO        
        self.data_avatars, self.stats = self.__dataset_info(self.src_scv_path, self.data_items)

        #self.classes = {'ON': ["0", "1"], 'OFF': ["0", "1"], 'ON/OFF': ["0", "1"]}
        self.labels = ['rl']

        # 3. Create Data Reader and Batch Converter
        self.data_reader = DataReader(self.theme)


    def __getitem__(self, index):
        try:
            input, label = self.cache[index]
        except:
            input = {}
            if "seq" in self.input_items:
                input["seq"] = (self.data_avatars["name"][index], self.data_avatars["seq"][index][0].replace("T", "U"))

            if "ss-rnafm" in self.input_items:
                input["ss-rnafm"] = self.generate_ss_embedding(
                    self.data_reader.read_embedding(self.data_avatars["ss-rnafm"][index]),
                    self.data_avatars["seq"][index][0].replace("T", "U"))
            label = {"rl": self.data_avatars["rl"][index],}
            if self.use_cache == 1:
                self.cache[index] = (input, label)

        return input, label

    def __len__(self):
        return self.stats.shape[0]

    def __dataset_info(self, src_csv_path, data_items):
        """
        :param name_path: txt record name list for specific set_name
        :param data_dir:
        :param msa_dir:
        :param ann1d_dir:
        :param ann2d_dir:
        :return:
        """
        seq_col = ['utr100']
        label_col = ['rl']  # need scale

        src_df = pd.read_csv(src_csv_path)   #, index_col=0)
        src_df.loc[:, "ori_index"] = src_df.index
        random_df = src_df[src_df['set'] == 'random']
        ## Filter out UTRs with too few less reads
        random_df = random_df[random_df['total_reads'] >= 10]    # 87000 -> 83919
        random_df['utr100'] = random_df['utr'] # + 75 * 'N'  # left padding from reference code
        #random_df['utr100'] = random_df['utr100'].str[-100:]
        random_df.sort_values('total_reads', inplace=True, ascending=False)
        random_df.reset_index(inplace=True, drop=True)

        # human set for evaluation too
        human_df = src_df[src_df['set'] == 'human']
        ## Filter out UTRs with too few less reads
        human_df = human_df[human_df['total_reads'] >= 10]   # 16739 -> 15555
        human_df['utr100'] = human_df['utr'] #+ 75 * 'N'  # left padding from reference code, here we use right padding
        #human_df['utr100'] = human_df['utr100'].str[-100:]
        human_df.sort_values('total_reads', inplace=True, ascending=False)
        human_df.reset_index(inplace=True, drop=True)

        random_df_test = pd.DataFrame(columns=random_df.columns)
        for i in range(25, 101):
            tmp = random_df[random_df['len'] == i]
            tmp.sort_values('total_reads', inplace=True, ascending=False)
            tmp.reset_index(inplace=True, drop=True)
            random_df_test = random_df_test.append(tmp.iloc[:100])

        human_df_test = pd.DataFrame(columns=human_df.columns)
        for i in range(25, 101):
            tmp = human_df[human_df['len'] == i]
            tmp.sort_values('total_reads', inplace=True, ascending=False)
            tmp.reset_index(inplace=True, drop=True)
            human_df_test = human_df_test.append(tmp.iloc[:100])

        train_df = pd.concat([random_df, random_df_test]).drop_duplicates(keep=False)  # 去重后 76319
        self.scaler = preprocessing.StandardScaler()
        self.scaler.fit(train_df.loc[:, label_col].values.reshape(-1, 1))
        train_df.loc[:,'scaled_rl'] = self.scaler.transform(train_df.loc[:, label_col].values.reshape(-1, 1))
        random_df_test.loc[:, 'scaled_rl'] = self.scaler.transform(random_df_test.loc[:, label_col].values.reshape(-1, 1))
        human_df_test.loc[:, 'scaled_rl'] = self.scaler.transform(human_df_test.loc[:, label_col].values.reshape(-1, 1))

        if self.set_name == "train":
            set_df = train_df
        elif self.set_name == "valid":
            set_df = random_df_test
        else:
            set_df = human_df_test

        seq = set_df[seq_col].values
        scaled_rl = set_df['scaled_rl'].values
        selected_indices = set_df["ori_index"].values

        self.name = selected_indices

        data_paths = {"name": self.name}
        for itemname in data_items:
            # input
            if itemname == "seq":
                data_paths[itemname] = seq
            elif itemname == "emb-rnafm":
                pass
            # label
            elif itemname == "rl":
                data_paths[itemname] = scaled_rl
            else:
                raise Exception("Unknown data item name {}".format(itemname))


        print("{} Dataset Info:".format(self.set_name))
        print("Length-Frequency Table")
        print(set_df["len"].describe())  # value_counts())

        return data_paths, set_df
    
class DataReader(object):
    def __init__(self, theme):
        """
        :param theme: "protein", "rna", "dna"
        :param data_type: "seq", "msa", "cov", "seq+msa", "seq+cov"   # input is controled
        :param msa_nseq: Reads the first nseq sequences from an MSA file if it is in "msa" data type.
        Notes:
        1.cache
        actually we should use dict for the data.
        data ->
        """
        # This is an efficient way to delete lowercase characters and insertion characters from a string
        deletekeys = dict.fromkeys(string.ascii_lowercase)
        deletekeys["."] = None
        deletekeys["*"] = None
        self.translation = str.maketrans(deletekeys)
        self.theme = theme        


    def read_embedding(self, emb_path):
        #pre, ext = os.path.splitext(emb_path)
        #if ext == ".npy":
        embedding = np.load(emb_path)
        return embedding
    
class BatchConverter(object):
    """
    Callable to convert an unprocessed (labels + strings) batch to a processed (labels + tensor) batch.
    """
    def __init__(self, alphabet, data_type="seq",):
        """
        :param alphabet:
        :param data_type: seq, msa
        """
        self.alphabet = alphabet
        self.data_type = data_type.split("+")

    def __call__(self, raw_data, raw_anns=None):
        """
        :param raw_data: each element in raw data should contain (description, seq)
        :param raw_anns:
        :return:
        """
        # creat a new batch of data tensors
        data = {}
        for key in raw_data.keys():
            if key == "seq":
                labels, strs, tokens = self.__call_seq__(raw_data["seq"])

                data["description"] = labels
                data["string"] = strs
                data["token"] = tokens
                data["depth"] = [1] * len(strs)
                data["length"] = [len(s) for s in strs]
            else:
                if isinstance(raw_data[key][0], str):
                    data[key] = raw_data[key]
                elif isinstance(raw_data[key][0], np.ndarray):
                    try:   # same length
                        data[key] = torch.Tensor(raw_data[key])
                    except:
                        # here we padding them with 0 for consistance with cnn's padding, which is different with ann's padding
                        data[key] = torch.Tensor(self.__padding_numpy_matrix__(raw_data[key], data["length"], pad_idx=0))
                elif isinstance(raw_data[key][0], float) or isinstance(raw_data[key][0], int):
                    data[key] = torch.Tensor(raw_data[key])


        # creat a new batch of ann tensors
        if raw_anns is not None:
            anns = {}
            for key in raw_anns.keys():
                if isinstance(raw_anns[key][0], str):
                    anns[key] = raw_anns[key]
                elif isinstance(raw_anns[key][0], np.ndarray):
                    try:   # same length
                        anns[key] = torch.Tensor(raw_anns[key])
                    except:
                        anns[key] = torch.Tensor(self.__padding_numpy_matrix__(raw_anns[key], data["length"]))
                elif isinstance(raw_anns[key][0], float) or isinstance(raw_anns[key][0], int):
                    anns[key] = torch.Tensor(raw_anns[key])
        else:
            anns = None

        return data, anns

    def __call_seq__(self, raw_batch: Sequence[Tuple[str, str]]):
        # RoBERTa uses an eos token, while ESM-1 does not.
        batch_size = len(raw_batch)
        max_len = max(len(seq_str) for _, seq_str in raw_batch)
        tokens = torch.empty(
            (
                batch_size,
                max_len
                + int(self.alphabet.prepend_bos)
                + int(self.alphabet.append_eos),
            ),
            dtype=torch.int64,
        )
        tokens.fill_(self.alphabet.padding_idx)
        labels = []
        strs = []

        for i, (label, seq_str) in enumerate(raw_batch):
            labels.append(label)
            strs.append(seq_str)
            if self.alphabet.prepend_bos:
                tokens[i, 0] = self.alphabet.cls_idx
            seq = torch.tensor(
                [self.alphabet.get_idx(s) for s in seq_str], dtype=torch.int64
            )
            tokens[
            i,
            int(self.alphabet.prepend_bos): len(seq_str)
                                            + int(self.alphabet.prepend_bos),
            ] = seq
            if self.alphabet.append_eos:
                tokens[
                    i, len(seq_str) + int(self.alphabet.prepend_bos)
                ] = self.alphabet.eos_idx

        return labels, strs, tokens
    
def LofD_to_DofL(raw_batch):
    """
    list of dict to dict of list
    :param raw_batch:
    :return:
    """
    batch_size = len(raw_batch)
    example = raw_batch[0]
    new_batch = {}
    for key in example.keys():
        new_batch[key] = []
        for i in range(batch_size):
            new_batch[key].append(raw_batch[i][key])
    return new_batch

def build_collate_fn(alphabet, data_type):
    batch_converter = BatchConverter(alphabet, data_type)
    def collate_fn(batch):
        if len(batch[0]) == 1:
            data = zip(*batch)
            data = LofD_to_DofL(data)
            data, anns = batch_converter(data)
            anns = None
        elif len(batch[0]) == 2:
            data, anns = zip(*batch)
            data = LofD_to_DofL(data)
            anns = LofD_to_DofL(anns)
            data, anns = batch_converter(data, anns)
        else:
            raise Exception("Unexpected Num of Components in a Batch")
        return data, anns

    return collate_fn

```

## Data processing


```python
root_path = "./"
train_set =  Human_5Prime_UTR_VarLength(root=root_path, set_name="train", input_items=input_items, )
val_set =  Human_5Prime_UTR_VarLength(root=root_path, set_name="valid", input_items=input_items)
test_set =  Human_5Prime_UTR_VarLength(root=root_path, set_name="test", input_items=input_items)

collate_fn = build_collate_fn(alphabet, train_set.data_type)

num_workers = 4
train_batch_size = 64
train_loader = DataLoader(
    train_set, batch_size=train_batch_size, sampler=RandomSampler(train_set, replacement=False),
    num_workers=num_workers, collate_fn=collate_fn, drop_last=False
)

val_batch_size = train_batch_size * 4
val_loader = DataLoader(
    val_set, batch_size=val_batch_size, sampler=RandomSampler(val_set, replacement=False),
    num_workers=num_workers, collate_fn=collate_fn, drop_last=False
)

test_batch_size = train_batch_size * 4
test_loader = DataLoader(
    test_set, batch_size=test_batch_size, sampler=RandomSampler(test_set, replacement=False),
    num_workers=num_workers, collate_fn=collate_fn, drop_last=False
)

```

    /user/liyu/miniconda3/envs/jychen/lib/python3.8/site-packages/pandas/util/_decorators.py:311: SettingWithCopyWarning: 
    A value is trying to be set on a copy of a slice from a DataFrame
    
    See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
      return func(*args, **kwargs)


    train Dataset Info:
    Length-Frequency Table
    count     76319
    unique       76
    top          25
    freq       1058
    Name: len, dtype: int64


    /user/liyu/miniconda3/envs/jychen/lib/python3.8/site-packages/pandas/util/_decorators.py:311: SettingWithCopyWarning: 
    A value is trying to be set on a copy of a slice from a DataFrame
    
    See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
      return func(*args, **kwargs)


    valid Dataset Info:
    Length-Frequency Table
    count     7600
    unique      76
    top         25
    freq       100
    Name: len, dtype: int64


    /user/liyu/miniconda3/envs/jychen/lib/python3.8/site-packages/pandas/util/_decorators.py:311: SettingWithCopyWarning: 
    A value is trying to be set on a copy of a slice from a DataFrame
    
    See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
      return func(*args, **kwargs)


    test Dataset Info:
    Length-Frequency Table
    count     7600
    unique      76
    top         25
    freq       100
    Name: len, dtype: int64


# 3. Training Model


```python
n_epoches = 50
for i_e in range(n_epoches):
    all_loss = 0
    n_sample = 0
    iters = len(train_loader)

    for index, (data, anns) in enumerate(train_loader):
        #print(data)
        #print(anns)
        x = data["token"]
        results = backbone(x, need_head_weights=False, repr_layers=[12], return_contacts=False)
    
        #print(results)
        if "seq" in input_items and "emb-rnafm" in  input_items:
            input = {"token":x, "embedding": results["representations"][12]}
            results["rl"] = utr_func_predictor(x, input)
        #print(results["rl"])
    
        loss = criterion(results["rl"], anns["rl"])
        #print(loss)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
    
        all_loss = all_loss + loss.item() * train_batch_size
        n_sample = n_sample + x.shape[0]
        if index % 1 == 0:
            print("epoch: {}, iter:{}/{} MSE loss: {}".format(i_e, index, iters, all_loss/n_sample ))
    
        #break
        if index > 10:
            break
    #break
```

    epoch: 0, iter:0/1193 MSE loss: 0.9327887892723083
    epoch: 0, iter:1/1193 MSE loss: 0.9804719388484955
    epoch: 0, iter:2/1193 MSE loss: 0.9188532630602518
    epoch: 0, iter:3/1193 MSE loss: 1.0563729256391525
    epoch: 0, iter:4/1193 MSE loss: 1.067462933063507
    epoch: 0, iter:5/1193 MSE loss: 1.0521654784679413
    epoch: 0, iter:6/1193 MSE loss: 1.0582632252148219
    epoch: 0, iter:7/1193 MSE loss: 1.0355761870741844
    epoch: 0, iter:8/1193 MSE loss: 1.060308833916982
    epoch: 0, iter:9/1193 MSE loss: 1.0794333279132844
    epoch: 0, iter:10/1193 MSE loss: 1.0754052888263355
    epoch: 0, iter:11/1193 MSE loss: 1.0797414431969325
    ...



```python

```
